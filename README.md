# low_bit_readlist
## tect report
- nvidia:
<https://developer.nvidia.com/zh-cn/blog/nvidia-gpu-fp8-training-inference/><br/>

## net
- The Era of 1-bit LLMs: All Large Language Models are in 1.58 Bits<br/>
<https://arxiv.org/abs/2402.17764><br/>

- BitNet: Scaling 1-bit Transformers for Large Language Models<br/>
<https://arxiv.org/abs/2310.11453><br/>

- RetNet<br/>
<https://arxiv.org/pdf/2307.08621><br/>
- Mamba<br/>
<https://arxiv.org/abs/2312.00752><br/>
- GLA<br/>
<https://arxiv.org/abs/2312.06635><br/>
- RWKV<br/>
<https://arxiv.org/abs/2305.13048><br/>
- TransnormerLLM<br/>
<https://arxiv.org/abs/2307.14995><br/>

## optimizer
- Revisiting Zeroth-Order Optimization for Memory-Efficient LLM Fine-Tuning: A Benchmark<br/>
<https://arxiv.org/abs/2402.11592><br/>
<https://github.com/ZO-Bench/ZO-LLM><br/>

## quantization
QAT: <br/>

BitNet (binary version): https://arxiv.org/abs/2310.11453 <br/>

BitNet (ternary version): https://arxiv.org/abs/2402.17764 <br/>

GitHub: https://github.com/kyegomez/BitNet <br/>

OneBit: https://arxiv.org/abs/2402.11295  <br/>

Github: https://github.com/xuyuzhuang11/OneBit  <br/>

PTQ: <br/>

BiLLM: https://arxiv.org/abs/2402.04291  <br/>

FQT: <br/>

Training Transformers with 4-bit Integers: https://arxiv.org/abs/2306.11987  <br/>

GitHub: https://github.com/xijiu9/Train_Transformers_with_INT4 <br/>

 

Zhen-Dong/Awesome-Quantization-Papers: List of papers related to neural network quantization in recent AI conferences and journals. (github.com) <br/>



- Matmul or No Matmal in the Era of 1-bit LLMs<br/>
<https://arxiv.org/pdf/2408.11939><br/>

- Exploring Extreme Quantization in Spiking Language Models<br/>
<https://arxiv.org/abs/2405.02543><br/>

## cuda kernel, FPGA
- MatMul-free LM:
<https://mp.weixin.qq.com/s/vXU7u13tFGapUoI-ih485A><br/>
<https://arxiv.org/abs/2406.02528><br/>

- Ladder: Enabling Efficient Low-Precision Deep Learning Computing through Hardware-aware Tensor Transformation<br/>
<https://www.usenix.org/conference/osdi24/presentation/wang-lei><br/>

- LLMMMM: Large Language Models Matrix-Matrix Multiplications Characterization on Open Silicon<br/>
[poster]
<https://hal.science/hal-04592229/><br/>

- Near-Memory Processing for Low-precision Deep Neural Networks<br/>
<https://www.repository.cam.ac.uk/items/cf64e959-1ec5-453e-900a-e16f1f5eefc1><br/>


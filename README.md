# low_bit_readlist
Ladder: Enabling Efficient Low-Precision Deep Learning Computing through Hardware-aware Tensor Transformation<br/>
<https://www.usenix.org/conference/osdi24/presentation/wang-lei><br/>

The Era of 1-bit LLMs: All Large Language Models are in 1.58 Bits<br/>
<https://arxiv.org/abs/2402.17764><br/>

Matmul or No Matmal in the Era of 1-bit LLMs<br/>
<https://arxiv.org/pdf/2408.11939><br/>

Exploring Extreme Quantization in Spiking Language Models<br/>
<https://arxiv.org/pdf/2408.11939><br/>

Near-Memory Processing for Low-precision Deep Neural Networks<br/>
<https://www.repository.cam.ac.uk/items/cf64e959-1ec5-453e-900a-e16f1f5eefc1><br/>


LLMMMM: Large Language Models Matrix-Matrix Multiplications Characterization on Open Silicon<br/>
[poster]
<https://hal.science/hal-04592229/><br/>


# low_bit_readlist
## tect report
- SiliconCloud:
<https://mp.weixin.qq.com/s/vXU7u13tFGapUoI-ih485A><br/>
<https://arxiv.org/abs/2406.02528><br/>

- nvidia:
  <https://developer.nvidia.com/zh-cn/blog/nvidia-gpu-fp8-training-inference/><br/>
## paper
- Ladder: Enabling Efficient Low-Precision Deep Learning Computing through Hardware-aware Tensor Transformation<br/>
<https://www.usenix.org/conference/osdi24/presentation/wang-lei><br/>

- The Era of 1-bit LLMs: All Large Language Models are in 1.58 Bits<br/>
<https://arxiv.org/abs/2402.17764><br/>

- Matmul or No Matmal in the Era of 1-bit LLMs<br/>
<https://arxiv.org/pdf/2408.11939><br/>

- Exploring Extreme Quantization in Spiking Language Models<br/>
<https://arxiv.org/abs/2405.02543><br/>

- Near-Memory Processing for Low-precision Deep Neural Networks<br/>
<https://www.repository.cam.ac.uk/items/cf64e959-1ec5-453e-900a-e16f1f5eefc1><br/>


- LLMMMM: Large Language Models Matrix-Matrix Multiplications Characterization on Open Silicon<br/>
[poster]
<https://hal.science/hal-04592229/><br/>


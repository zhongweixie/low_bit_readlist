# low_bit_readlist
## tect report
- nvidia:
<https://developer.nvidia.com/zh-cn/blog/nvidia-gpu-fp8-training-inference/><br/>

## net
- The Era of 1-bit LLMs: All Large Language Models are in 1.58 Bits<br/>
<https://arxiv.org/abs/2402.17764><br/>

- BitNet: Scaling 1-bit Transformers for Large Language Models<br/>
<https://arxiv.org/abs/2310.11453><br/>

-RetNet(https://arxiv.org/pdf/2307.08621)<br/>
-Mamba(https://arxiv.org/abs/2312.00752)<br/>
-GLA(https://arxiv.org/abs/2312.06635)<br/>
-RWKV(https://arxiv.org/abs/2305.13048)<br/>
-TransnormerLLM(https://arxiv.org/abs/2307.14995)<br/>

## optimizer
- Revisiting Zeroth-Order Optimization for Memory-Efficient LLM Fine-Tuning: A Benchmark<br/>
<https://arxiv.org/abs/2402.11592><br/>
<https://github.com/ZO-Bench/ZO-LLM><br/>

## quantization
- Matmul or No Matmal in the Era of 1-bit LLMs<br/>
<https://arxiv.org/pdf/2408.11939><br/>

- Exploring Extreme Quantization in Spiking Language Models<br/>
<https://arxiv.org/abs/2405.02543><br/>

## cuda kernel, FPGA
- MatMul-free LM:
<https://mp.weixin.qq.com/s/vXU7u13tFGapUoI-ih485A><br/>
<https://arxiv.org/abs/2406.02528><br/>

- Ladder: Enabling Efficient Low-Precision Deep Learning Computing through Hardware-aware Tensor Transformation<br/>
<https://www.usenix.org/conference/osdi24/presentation/wang-lei><br/>

- LLMMMM: Large Language Models Matrix-Matrix Multiplications Characterization on Open Silicon<br/>
[poster]
<https://hal.science/hal-04592229/><br/>

- Near-Memory Processing for Low-precision Deep Neural Networks<br/>
<https://www.repository.cam.ac.uk/items/cf64e959-1ec5-453e-900a-e16f1f5eefc1><br/>

